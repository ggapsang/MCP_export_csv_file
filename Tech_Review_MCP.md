---
tags:
  - 인공지능
  - AI
  - LLM
  - MCP
  - Model_Context_Protocol
---
# 개요 - MCP

- Claude AI를 개발한 Anthrophic이 주도하여 2024년 11월 처음 공개되었다. 공개 초기에는 큰 반응이 없었으나 AI 모델이 실시간 데이터베이스를 조회하는 등의 사례들이 주목을 받으면서, 다양한 AI 플랫폼과 도구들이 MCP를 지원하기 시작하여 생태계가 빠르게 확장되었다.
- 2025-03-26에는 OpenAI가 자신들의 어플리케이션(chatGPT 웹, 앱)에 MCP 표준을 채택할 것을 결정하면서 사실상 업계 표준으로 굳어질 것으로 예상된다.

MCP(Model Context Protocol) : 대규모 언어 모델(LLM)과 외부 데이터/도구를 연결해주는 표준화된 방식


---

# Before MCP, After MCP

## Without MCP

예를 들어, text2SQL의 경우 유저가 AI에게 "2021년 이후 설치된 설비 리스트를 보여줘"라고 요청할 경우, 프로그램은 내부적으로 다음과 같은 단계를 거쳐 작동해야 했다.

1. 아무 설정도 해 놓지 않았을 경우 LLM은 해당 데이터를 가지고 있지 않고, 따라서 응답이 불가능하거나 잘못된 메시지를 생성함
2. 이 때문에 사용자나 AI 어플리케이션 개발자는 직접
    1. 설비 데이터를 가져와서
    2. 특정 포멧으로 정리한 후
    3. 어플리케이션(챗봇 창)에 직접 복사 붙여넣기를 하거나 다른 경로로 업로드 하는 방식으로 AI 에이전트에게 직접 데이터를 제공함
3. 2번의 과정을 거친 후에야 AI 에이전트가 text2SQL을 만들게 됨
4. 만약 여기서 더 나아가 추가적인 작업(데이터 분석, 특정 데이터 수정 등)을 원한다면 사용자는 다시 이를 지시하고
5. AI 에이전트는는 유저의 지시를 실행하기 위한 다양한 방법들을 '제시' 해줌(SQL쿼리를 짜준다던가, 데이터 분석을 위한 소스코드나 분석 결과를 텍스트로 생성함)
6. 개발자는 AI가 이렇게 만든 결과물을 실제 데이터베이스에 적용하기 위한 API등을 추가로 구성해야 하고, 사용자는 이를 이용하거나 AI 에이전트가 만들어준 결과를 가지고 직접 실행시켜야 함

## With MCP

1. AI는 데이터베이스 혹은 외부에 연결된 도구(Google Sheets 등 상용 어플리케이션도 가능)에 MCP를 통해 외부데이터나 어플리케이션에 스스로 직접 접근
2. MCP 서버를 통해 관련 데이터를 직접 가져오고, 필요시 사전에 정의된 툴을 통하여 자동 분석하거나 데이터를 수정할수도 있음
3. AI와 툴 사이의 연동이 표준화된 방식(MCP)로 이루어지기 때문에 구성이 쉬워지고, 사용자의 지시는 최소화되면서 AI가 능동적으로 작업을 수행함

## 그렇다면 MCP는 API의 일종인가?

MCP도 API의 한 형태이며, API의 표준적인 정의에 부합한다. 우선 기존의 MCP는 API라 볼 수 있는 분명한 특징이 있는데

  

1. MCP와 API는 둘다 AI(또는 앱)와 외부 서비스를 연결하는 인터페이스이고
2. Request-Response 방식을 사용하며
3. 주로 JSON 기반 구조로 데이터를 주고받는다

  

다만 API의 사용 주체가 사람이 아니라 AI라는 점에서 기존의 API와 분명한 차이점도 있다. 기존 API는 개발자가 정해진 프로그램 로직에 따라 명시적으로 호출하지만, MCP는 AI가 사용자의 요청사항을 해석해 어떤 기능이나 데이터가 필요한지를 스스로 판단한 후 자동으로 호출 여부를 결정하고 실행한다. 즉 기존에 API들은 특정 상황에서 어떤 API를 사용할지를 전부 개발자가 일일이 정의했다면, MCP의 경우 여러 MCP로 제공되는 기능들 중 LLM이 현재 프롬프트 문맥에 가장 적절한 도구를 판단해 선택하고 호출한다. 또한 '프로토콜'이라는 표현에서 강조되듯이, 다양한 형태로 존재하는 기존 API는 포맷이 제각각인 반면, MCP는 JSON 기반의 통일된 구조화된 'tool schema'를 따른다.

## MCP는 플러그인의 일종인가?

유저 입장에서 본다면 플러그인 시스템과 매우 유사하다. 'AI가 사용자의 질문에 답하기 위해 필요할 때 외부 기능(도구)을 불러다 쓰는 구조'이기 때문이다. 다만 위에 설명했듯이 직접적으로 MCP를 사용하는 주체는 사람이 아닌 AI이며, 어떤 '플러그인'을 사용할지를 AI가 유저의 지시를 받고 스스로 판단한다는 점에서 일반 플러그인이랑 차이점이 있을 뿐이다.

## MCP가 혁신적이거나 기술적으로 완전히 새로운 개념은 아니다

- API 호출 : 이미 오래된 개념
- 플러그인 구조 : 오래 전부터 있어 왔음

그러므로 단순히 개념만 놓고 보면 "AI가 쓸 수 있는 API를 포멧에 맞춰서 만든 것", 본질적으로 그 이상도 이하도 아니다

그렇지만 왜 주목받고 있는가라는 질문에 대해서, MCP의 가치는 그 혁신성보다는 "무엇을 가능하게 하느냐"에 있기 때문이다

|Before MCP|After MCP|
|---|---|
|AI는 대화만 하고 실제 행동은 할 수 없음|AI가 직접 툴을 선택하고 작업 수행|
|API 연결은 사람이 결정|AI가 판단해서 외부 시스템과 연동|
|플러그인 관리 복잡|통일된 스펙(MCP)로 보다 편리하게 구성|

MCP는 기술적 혁신이라기보다는 제도적 변화(운영 구조의 전환)에 있다.

예) 기술적 혁신 : 새로운 도구가 나옴(기차의 발명)

제도적 개혁 : 새로운 도구를 누구나 표준화된 방식으로 쓸 수 있게 만든 것(철도 시스템의 표준화)

---

# MCP의 기본 개념 및 역할

다시 MCP의 정의를 설명하면,

AI 모델(에이전트)와 외부 데이터소스(또는 도구)를 연결해주는 개방형 표준 프로토콜

LLM 모델이 답변에 필요한 맥락(Context)이나 정보를 외부에서 가져올 수 있게 해주는 일종의 통신 통로

MCP는 AI 분야의 'USB C' 포트로 불리기도 한다. LLM 모델은 본질적으로 인풋에 대한 아웃풋('텍스트'로 대표되는 일종의 신호) 생성기이며, 따라서 사용자가 직접적으로 인풋에서 제공한 텍스트 외에 별도의 데이터나 지식에 접근할 수 없다(사전에 학습된 내용이 아니라면). 만약 새로운 데이터나 지식을 LLM에 반영하려고 하면 데이터 원본에 대한 별도의 전처리 및 커스텀이 반드시 필요했다.

즉 기존의 LLM을 실용화하려고 할 때 부딪치는 주요한 문제점은

1. LLM이 학습한 지식의 한계와 업데이트의 어려움
    1. LLM은 훈련 데이터에 포함된 정보만을 알고 있음
    2. 새로운 지식을 주입하기 위해 LLM을 훈련하는 과정에 엄청난 시간과 자원이 소모됨
2. 전문 도메인 지식 부족
    1. LLM은 공개적으로 이용 가능한 일반적인 데이터를 사용하여 훈련됨
    2. 특정 도메인에 대한 전문 지식, 내부 프로세스 등은 훈련 범위에 포함되지 않음
3. 외부 데이터에 엑세스할 통일된 표준이 없음
    1. LLM에 추가 정보(`context`)를 제공하는 많은 방법들이 있음(대표적인 예가 RAG). 그렇지만 다양한 외부 기능과 데이터를 활용해야 할 경우, 이들을 하나의 AI 어플리케이션 안에 묶기 위해서는 많은 비용과 시간이 필요하며, 이렇게 통합된 결과물에 공통된 표준화 방법이 없기 때문에 재사용이 어려움

1과 2를 극복하는 방식이 기존에는 파인튜닝(전이학습)이었고, 추가적인 `context`를 제공하기 위한 RAG와 같은 기술이 주목받았다. 그러나 MCP가 현재 사실상의 업계 표준이 되어 가고 있는 상황에서, LLM이 외부 데이터/도구에 직접 접근할 수단이 표준화되면 이런 노력에 들어가는 비용들이 크게 감소하게 된다.

MCP의 등장과 보편화로 얻게 된 이점들에 대해서는 크게 다음과 같이 정리할 수 있다.

1. 미리 개발된 풍부한 참조 자료 : 현재 파일 시스템, 데이터베이스(PostgreSQL, SQLite), 개발도구(Git, Github), 네트워크(Fetch API) 등에 대해 사전에 제작되어 공개된 MCP 서버들이 있다
2. LLM 모델 간의 유연한 전환 : 어제까지 GPT-4를 사용하다, 오늘 llama를 쓰고, 내일 Claude나 Gemini를 사용하는 방식이 가능해진다
3. 복잡한 AI 워크플로우 구축이 더 쉬워질 수 있다

  

## MCP의 주요 구성 요소(Context, Model, Protocol)

MCP 역시 API 통신의 일종이며, 가장 대표적인 API 통신 방식 중 하나인 서버-클라이언트 구조를 따른다.

### MCP 호스트(Host)

AI 모델을 운용하는 메인 어플리케이션. Claude AI 데스크탑 버전, Cursor AI나 VS code의 copilot 등이 이러한 어플리케이션의 대표적인 예시이다. 사용자로부터 질문이나 명령을 직접 받아 모델에 전달하고, 응답을 사용자에게 보여주며, 사용자와 AI모델 사이에서 상호작용을 담당한다.

### MCP 클라이언트(Client)

호스트 어플리케이션(AI 어플리케이션)과 연결되어, 내부에서 동작한다. 하나의 MCP 서버와 1대 1로 연결되어 있다. 반복해서 이야기하듯이, MCP는 외부 데이터/어플리케이션과 통신하는 일종의 API다. 따라서 당연하게도 사용자의 명령이 어떤 것이냐에 따라, 이를 처리하기 위해 하나 이상의 외부 어플리케이션이나 데이터가 필요할 수 있다. 이러한 외부 데이터나 기능을 제공하는 주체가 MCP 서버이며, MCP 클라이언트는 각각의 MCP 서버들을 전담하면서 필요한 경우 서버로 요청을 보내고 응답을 받아 모델에 전달한다.

### MCP 서버(Server)

MCP 클라이언트의 요청을 받아 외부 데이터나 기능을 제공한다. MCP 서버는 자신의 역할에 따라 다양한 기능을 수행할 수 있다(파일 시스템, 데이터베이스 제어, 웹브라우저 조작). MCP 서버는 자신이 제공하는 기능을 표준화된 인터페이스로 정의하여 MCP 클라이언트의 요청에 응답한다.

### Context(맥락)

MCP에서 context란 단순히 '맥락'이라는 언어적 의미보다 더 구체적이다. MCP에서 `context`라고 하면, MCP 서버가 제공하는 실제 데이터와 도구들을 통칭한다. 이 도구들은 다음과 같이 분류한다

- 리소스(Resource) : AI모델이 참고할 수 있는 읽기 전용 데이터. 고유한 URL형식으로 식별된다(http 통신의 `GET`요청과 유사함)
- 도구(Tools) : AI모델이 호출할 수 있는 기능 또는 함수
- 프롬프트(Prompt) : AI 모델에게 특정 지시나 템플릿을 제공하는 문구. 주로 복잡한 상호작용을 패턴화할때 사용한다

### 프로토콜(Protocol)

MCP의 통신 규약. JSON 형식으로 데이터를 주고받는다(JSON-RPC 2.0). 아래에서 자세히 살펴보겠지만 MCP 클라이언트와 서버가 주고받는 각 메세지에는 `id`, `method`, `params`등이 주요한 요소로 포함된다.

### MCP 구조 개요

```Mermaid
graph LR;
    User <--> Host;
    subgraph Host
        LLM_model;
        subgraph LLM_model
            infr(inference);
        end
        infr <--> MCP_client_1;
        infr <--> MCP_client_2;
    end
    MCP_client_1 <--> MCP_server_1;
    MCP_client_2 <--> MCP_server_2;
    MCP_server_1 <--> Application;
    MCP_server_2 <--> Database;        
```

  

---

# MCP의 동작 방식과 흐름

## 예시 : MCP 서버에서 tool과 resource를 간단히 정의하기

### Tools 정의 예시

```Python
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("Demo") # MCP 서버 인스턴스 생성

@mcp.tool()
def database_format(agent_request : bool, database_name) -> str :
  """데이터베이스를 삭제한다"""
  if agent_request :
    db_controller.drop_database(database_name)
    return f"삭제완료"
  else
    return ""
```

### Resource 정의 예시

```Python
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("Demo") # MCP 서버 인스턴스 생성

@mcp.resource("basic_info://{name}")
def get_greeting(name : str) -> str :
  """해당 이름을 가진 인물 정보를 가져온다"""
  age = get_age_from_name(name)
  job = get_age_from_name(name)  
  return  f"이름 : {name}, 나이 : {name}, 직업 : {job}"
```

## MCP의 동작 단계

### 1. 초기화(Initialize)

MCP 클라이언트가 서버에 연결을 시작할 때, 가장 먼저 초기화 메세지를 보낸다. 초기화 메세지에는 사용 가능한 MCP 프로토콜 버전 등과 같은 클라이언트 정보가 담겨 있다. 서버는 이를 받아서 자신이 지원하는 기능과 버전을 응답하여 이른바 '호환성 교섭'을 진행한다. 이 단계를 거친 후 클라이언트-서버 세션이 열린다.

### 2. 기능 협상 및 발견

세션이 열린 후, 클라이언트는 서버가 어떠한 `context`를 제공하는지 알아본다. 예를 들어 서버가 어떤 `tool`을 가지고 있는지 알고 싶다면, 클라이언트는 `"tool/list"`매서드의 JSON-RPC 요청을 서버로 보낸다. 그러면 서버는 자신이 보유한 `tool`들의 이름, 설명, 입력 스키마 등을 나열하여 응답한다.

```Json
{ 
    "jsonrpc": "2.0", 
    "id": 1, 
    "result": { 
        "tools": [ 
            { 
                "name": "database_format", 
                "description": "데이터베이스를 삭제해버린다", 
                "inputSchema": { 
                    "type": "object", 
                    "properties": { 
                            "agent_request": { "type": "bool" }, 
                            "database_name": { "type": "str" } }, 
                    "required": ["agent_request", "database_name"] 
                } 
            } 
        ] 
    } 
}
```

MCP 클라이언트는 이러한 요청을 AI 에이전트에 알려준다(예를 들어 시스템 프롬프트로 전달할 수 있다). 혹은 사용자에게 직접 보여주면서 사용자가 인터페이스를 선택하게 할 수도 있다.

### 3. 모델 요청 처리

사용자가 MCP 호스트에 질문이나 지시를 하면, MCP 호스트는 우선 AI모델(LLM)에 질문을 전달한다. 모델은 자신이 사용할 수 있는 `tool`과 `resource`목록을 context로 제공받았으므로, 사용자의 지시에 응답하기 위해 외부 도구들을 활용해야 될지 판단할 수 있게 된다. 예컨대, "저 db 보기 싫으니 다 날려버려" 이렇게 말하면 LLM은 "내가 가진 `tool`중에서 `database_format`이라는 `tool`이 있으니 그걸 사용해야겠군" 이라고 결론을 내릴 수 있다. 이때 모델은 1차적으로 `tool`을 어떻게 호출할지 결과만 만들어내며, 실제 호출은 하지 않는다. 모델의 출력은 어떤 도구를 어떤 인자로 써달라는 '요청'의 형태로 표현된다.

### 4. 호출 요청

모델이 만약 특정한 서버의 `tool`을 사용하고자 하면 호스트 MCP 클라이언트가 개입해 해당 `tool`호출을 실제로 수행하게 된다. 클라이언트는 모델의 지시에 따라 MCP 서버로 `"tool/call"` 요청을 보내며, 호출할 `tool`이름과 인자들을 JSON 형태로 전달한다. 예컨대 위에 정의된 데이터베이스 포멧 함수의 예시에서 보면

```Json
{
    jsonrpc : "2.0",
    "id" : 2,
    "method" : "tool/call",
    "params" : {
        "name" : "database_format",
        "arguments" : {
            "agent_request" : True,
            "database_name" : "pir_file"
        }
    }
}
```

MCP 서버는 요청을 받아 `database_format` 함수(`tool`)를 실행하고, 그 결과를 JSON 형태로 반환한다. 만약 성공이라면 `"result"` 필드에, 실패 시 `"error"` 필드에 응답이 오게 된다.

```Json
{
    jsonrpc : "2.0",
    "id" : 2,
    "result" : {
        "content" : [
            {
                "type" : "text",
                "text" : "삭제 완료",
                "annotations" : {"audience" : ["assistant"]}
            }
        ]
    }
}
```

여기서 `"annotations" : {"audience" : ["assistant"]}` 부분은 이 결과가 모델(assistant)에게만 보이는 context임을 나타낸다. 즉 사용자에게 직접 노출되지 않고 모델의 답변 생성에만 해당 결과값이 사용된다.

### 5. 모델 응답 생성

MCP 서버로부터 필요한 외부 데이터나 실행 결과를 받은 AI 에이전트의 LLM은 사용자에게 줄 최종 답변을 생성한다. MCP 서버는 "삭제 완료"라는 메세지만 보냈지만, 모델은 이를 참조하여, "말씀하신대로 DB를 삭제하였습니다" 등과 같이 모델 고유의 언어 생성 능력을 활용해, MCP로부터 얻은 `context`가 답변 내용에 반영된다.

### 6. 사용자에게 응답 전달

호스트 어플리케이션이 모델이 생성한 답변을 사용자에게 출력한다. 이렇게 해서 사용자의 질문에 대해 외부 정보를 활용한 답변이 완료된다

  

---

# Smithery 플랫폼, 그리고 MCP의 난점

# [https://smithery.ai/](https://smithery.ai/)

공개된 MCP 서버 플랫폼

  

[https://smithery.ai/server/mcp-server-sqlite-npx](https://smithery.ai/server/mcp-server-sqlite-npx)

이미 DB를 조작하고 제어하는 많은 MCP 서버들이 공개되어 있다

## 난점 : MCP 클라이언트쪽 구성이 더 어려움

MCP에 대해 찾아보다 보면 MCP 서버에 대한 설명, MCP 서버를 만드는 방법 등은 압도적으로 많지만, MCP 클라이언트 쪽 참조 자료는 거의 없다. 현재 MCP 서버는 누구나 만들 수 있지만, MCP 호스트 역할을 수행할 수 있는 "AI 어플리케이션"은 아무나 만들 수 없기 때문이다. 따라서 이러한 MCP가 완전히 표준으로 자리 잡게 되면, Claude, ChatGPT, Curosr 같은 기존 대형 플랫폼에 대한 의존도는 오히려 높아질 것으로 예상된다.

### 핵심은 항상 클라이언트

이는 MCP가 "도구 호출 표준"일 뿐, 주체는 항상 클라이언트이기 때문이다. 어떤 도구를 쓸지 "결정"하고 "호출 요청"을 보내는 쪽은 MCP 클라이언트이며, MCP 서버는 도구를 설명하고 실행 로직을 제공할 뿐이다. 즉 LLM이 들어 있는 AI 어플리케이션이 항상 메인일 수밖에 없다. 그런데 당연하게도 이러한 AI 어플리케이션을 만드는 것은 쉽지 않다. AI 어플리케이션(MCP 호스트)은 단순 LLM API 래퍼가 아니며, 프롬프트 관리, context injection, tool schema 핸들링, 응답 개입 등 '할 일'이 매우 많다. 결국 Open AI나 Anthropic, Curosr 등 소수 앱만 지금 가능한 일이다. 즉 MCP를 써보고 싶어도 누군가는 '클라이언트 역할을 해줄 녀석'이 없는 상황이다.

따라서 MCP의 명시적인 의도는 누구나 도구를 만들수 있게 해 두었지만, LLM을 사용할수 있는 어플리케이션은 소수이기 때문에, 오히려 중앙화된 LLM 플랫폼에 대한 종속이 심화될 수 있는 것이다.

다만 llama나 deepseek같이 무료로 공개된 LLM모델이 있기 때문에, 모델 학습 자체가 난점은 아니며, 더 중요한 문제는 LLM 모델과 MCP 클라이언트 사이의 상호작용을 어떻게 구성하느냐가 AI 어플리케이션 개발에 키가 된다. 구조적으로, LLM은 도구를 '선택'하고 MCP 클라이언트는 도구를 '실행'하는 관계에서 이 둘을 정확하게 이어주는 제어 흐름을 만드는 것이 관건이다.

### 상호작용 설계 시 고려점

- LLM이 도구를 "명확하게 식별할 수 있게" schema를 줘야 함
    - 이름, 설명, 파라미터 타입 명확하게 제공
    - 자연어 프롬프트에 스며들 수 있도록 prompt engineering도 포함
- MCP 클라이언트는 유연하게 다양한 도구를 실행할 수 있어야 함
    - `tool_name` → `tool_fn` 매핑 테이블 
    - 오류 핸들링 (missing params, wrong type 등)
- LLM <-> MCP <-> 사용자 간 상태 전이 관리
    - 도구 호출 중간에 사용자 질문이 다시 올 수 있음
    - 멀티턴 대화 흐름 제어 (ex: “잠깐, 범위 다시 지정해줘”)

  

---

# 결론 : 요약

AI가 직접 사용할 수 있는 API인 MCP가 업계 표준으로 자리잡고 있다

기존의 LLM이 '명령 생성기'의 역할이었다면 (이거 좀 만들어줘 -> 네 그건 이런 식으로 하면 됩니다), MCP를 통하여 AI가 사용자의 명령을 직접 수행할 수 있다(이거 만들어서 알아서 바꿔놔 -> 네 바꿔 놨습니다)

MCP를 통해서 모델이 직접적으로 맥락(context)을 찾고 활용할 수 있게 되면, 기존의 RAG라든지 파인튜닝 같은 기술의 위상도 달라진다. 조금 성능이 떨어지는 범용 LLM을 가지고 특정 분야에 특화시킨(특정 분야에 똑똑한 사람) AI 모델 보다는 뛰어난 범용 AI에 다양한 MCP 서버를 연결하는 것이 더 나을 수도 있다. 사람으로 비유하자면 보편적으로 아주 똑똑한 천재 한명에게, 그렇지만 해당 분야에 대해서는 잘 모르는 사람에게 일을 시킬 것인지, 각각의 분야에 특화되었지만 조금 능력이 떨어지는 여러 명을 두고 일을 시킬 것인지의 문제와 비슷하다. 현재 트렌드는 전자 쪽으로 가고 있다.

실제로 이미 파인튜닝 대신에 `tool + context window`로 해결하고 있고, RAG 대신 `external search`같은 방식이 더 유연하다고 인정받고 있다. 랭체인 같은 개념들도 기본적으로 "모델을 훈련하지 말고, 연결하라"는 철학 아래 구성되는 것이다. MCP는 이러한 흐름에 부합하며, 특별한 이유가 있지 않은 이상 '똑똑한 범용 AI + 잘 짜여진 도구'를 더 선호할 수밖에 없다.

따라서 AI 어플리케이션 개발은 AtoZ와 같은 방식을 가지기 보다는 '조립형 AI'들로 빠르게 원하는 기능을 구성하는 방식이 될 가능성이 더 높다.

물론 상황에 따라 범용 LLM이 불가능한 경우가 있다(보안, 프라이버시, 오프라인 환경). 그런 곳에서는 여전히 파인튜닝된 로컬 모델이 대세일 수 있다. 또한 [구성 요소 3.loop](https://plantinsight.sharepoint.com/:fl:/g/contentstorage/CSP_f8215b93-bc58-48bd-9f68-c5fa15249f93/EVDDOqVtOKtMk70Uw-tOANYBFB5wC7xZsQG4cz71M3xwJA?e=Xhr9YD&nav=cz0lMkZjb250ZW50c3RvcmFnZSUyRkNTUF9mODIxNWI5My1iYzU4LTQ4YmQtOWY2OC1jNWZhMTUyNDlmOTMmZD1iIWsxc2gtRmk4dlVpZmFNWDZGU1Nma3h1aVpoZEZSQ1pGdm5oWmhHNHZBLUoxaExCMEZySFNTWjFOYnN5ekZpTXkmZj0wMVNaNjZXMkNRWU01S0szSllWTkdKSFBJVVlQVlU0QUdXJmM9JTJGJmZsdWlkPTEmYT1Mb29wQXBwJnA9JTQwZmx1aWR4JTJGbG9vcC1wYWdlLWNvbnRhaW5lcg%3D%3D)에서 설명했듯이 파인튜닝은 원래부터도 모델이 특정한 테스크에 대해 '일정한 형식과 스타일로 출력하도록' 만드는데 있었지, 새로운 지식을 주입하는 과정은 아니었다(특정 도메인의 언어 사용 방식에 적응시키는 과정으로 보는 것이 더 적합함).

AI 개발의 조합(3-layer)

- MCP : AI 모델에 행동 등력 부여(execution layer)
- 파인튜닝 : 성격, 도메인, 말투, 정책 주입(identify layer)
- RAG : 지식 반입 경로(memory layer)

## 예시 아이디어 : 플랜트 운영지원 AI

### 시나리오

공정 엔지니어가 AI에게 장치 고장 보고서 작성을 요청하면, AI는 계기장치를 통해 수집된 계측 데이터를 분석하고, 각종 관련 문서를 참조해 보고서를 자동으로 작성한다

### 흐름

1. 사용자 프롬프트
2. 범용 LLM(Claude, Llama, GPT)
    1. 도구 판단 및 호출 -> [MCP를 통해 외부 시스템 실행(데이터조회, 표준절차서 검색, 보고서 생성기에 접근)]
    2. 장기 지식(도메인 특화 지식) 참조 -> [RAG : 기준 문서/운전 메뉴얼에서 검색]
    3. 조직에 맞는 표현/응답 스타일 -> [파인튜닝된 말투, 지식, 용어 반영]
3. 최종 응답 : 보고서 생성

### 2. a, b, c 단계에서 각 구성요소들의 역할(예시)

1. MCP (실행 도구 연결)
    1. 예시 도구
        1. `get_sensor_data(tag no : str, from : datetime, to : datetime)`
        2. `generate_report(input_data, : dict)`
    2. LLM이 필요시 `get_sensor_data`도구를 호출하여 설비번호, 압력, 온도 등을 가져오고 그걸 바탕으로 보고서를 생성하는 도구를 다시 호출
2. RAG (지식 검색)
    1. LLM이 외우고 있지 않은 문서 기반 지식 참조
        1. 운전 메뉴얼
        2. 예방정비 메뉴얼
        3. 유지보수 이력
        4. 품질관리 표준 규칙
    2. 프롬프트 문맥에서 필요한 내용을 추출하고 벡터 DB에서 유사 문서를 검색해 LLM에 함께 주입
3. 파인튜닝(스타일, 사내 지식 내면화)
    1. 회사 특유의 문체
    2. 자주 쓰는 고유 용어
    3. 보고서 작성 스타일 반영
    4. 이렇게 되면 동일한 데이터를 받아도
        1. 튜닝 안된 LLM : '해당 열교환기에서 기준치 이상의 이상값이 나옵니다. 정비가 필요할 것 같습니다.'
        2. 파인튜닝을 한다면 : "52-E-203" Floating Type HX에서 정상값 초과 압력 지속. 긴급 점검 필요"
    5. 와 같은 방식으로 문체를 바꿀 수 있음